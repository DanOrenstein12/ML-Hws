{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-983fd0697407>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-983fd0697407>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take the header out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    data = []\n",
    "    with open('train.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        headers = next(reader) # take the header out\n",
    "        for row in reader: # each row is a list\n",
    "            data.append(row)\n",
    "    data  = np.array(data, dtype = np.float)\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def read_test_data():\n",
    "    data = []\n",
    "    with open('test.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        headers = next(reader) # take the header out\n",
    "        for row in reader: # each row is a list\n",
    "            data.append(row)\n",
    "    data  = np.array(data, dtype = np.float)\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = read_data()\n",
    "X_test, y_test = read_test_data()\n",
    "array = np.zeros((y.size + 1, y.size + 1))\n",
    "\n",
    "def kernal(x_i,x_j, quant):\n",
    "    x = x_i - x_j\n",
    "    norm = np.linalg.norm(x)\n",
    "    final_val = math.exp(-quant * norm**2)\n",
    "    return final_val\n",
    "\n",
    "\n",
    "def populate(array, gamma, C):\n",
    "    for i in range(0, y.size):\n",
    "        array[i + 1][0] = 1\n",
    "        for j in range(0, y.size):\n",
    "            array[i + 1][j + 1] = kernal(X[j], X[i], gamma)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(1, array.shape[0]):\n",
    "        array[0][i] = 1\n",
    "        array[i][i] += (1/C)\n",
    "\n",
    "\n",
    "def predict(train_X, data, solution, C):\n",
    "    s = 0\n",
    "    for i in range(1,solution.size):\n",
    "        s += solution[i] * kernal(train_X[i-1], data, C)\n",
    "\n",
    "    s += solution[0]\n",
    "    if s >= 0:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = -1\n",
    "    return result\n",
    "\n",
    "def results(train_X, train_Y, test_X, test_Y, solution, gamma):\n",
    "    train_pred = np.zeros(train_Y.size)\n",
    "    test_pred = np.zeros(test_Y.size)\n",
    "    for i in range(train_Y.size):\n",
    "        train_pred[i] = predict(train_X, train_X[i], solution, gamma)\n",
    "\n",
    "    for i in range(test_Y.size):\n",
    "        test_pred[i] = predict(train_X, test_X[i], solution, gamma)\n",
    "\n",
    "    LSSVM_train_error = eval(train_pred, train_Y)\n",
    "    LSSVM_test_error = eval(test_pred, test_Y)\n",
    "    return LSSVM_train_error, LSSVM_test_error\n",
    "\n",
    "\n",
    "def eval(predicted, actual):\n",
    "    print(1-metrics.accuracy_score(actual, predicted))\n",
    "    return 1-metrics.accuracy_score(actual, predicted)\n",
    "\n",
    "\n",
    "\n",
    "# populate(array, 1, 1)\n",
    "# new_y = np.append(0,y)\n",
    "# solution = np.linalg.solve(array, new_y)\n",
    "# results(X,y, X_test,y_test,solution, 1)\n",
    "\n",
    "def LSSVM(X,y, X_test, y_test, gamma, C):\n",
    "    array = np.zeros((y.size + 1, y.size + 1))\n",
    "    populate(array, gamma, C)\n",
    "    new_y = np.append(0, y)\n",
    "    solution = np.linalg.solve(array, new_y)\n",
    "    error_train, error_test = results(X,y, X_test, y_test, solution, gamma)\n",
    "    return error_train, error_test\n",
    "\n",
    "LSSVM(X,y, X_test,y_test,10, 1)\n",
    "LSSVM(X,y, X_test,y_test,100, 1)\n",
    "LSSVM(X,y, X_test,y_test,1000, 1)\n",
    "LSSVM(X,y, X_test,y_test,10000, 1)\n",
    "\n",
    "\n",
    "def rbf_svm(X,y, X_test, y_test,gamma, c):\n",
    "    clf = svm.SVC(gamma=gamma, kernel='rbf', C=c)\n",
    "    clf.fit(X, y)\n",
    "    pred_train_reg=clf.predict(X)\n",
    "    y_pred_reg_test=clf.predict(X_test)\n",
    "    svm_train_error = 1-metrics.accuracy_score(y, pred_train_reg)\n",
    "    svm_test_error = 1-metrics.accuracy_score(y_test, y_pred_reg_test)\n",
    "    return svm_train_error, svm_test_error\n",
    "\n",
    "\n",
    "def linear_svm(X,y, X_test, y_test,gamma, c):\n",
    "    clf = svm.SVC(gamma=gamma, kernel='linear', C=c)\n",
    "    clf.fit(X, y)\n",
    "    pred_train_reg=clf.predict(X)\n",
    "    y_pred_reg_test=clf.predict(X_test)\n",
    "    svm_train_error = 1-metrics.accuracy_score(y, pred_train_reg)\n",
    "    svm_test_error = 1-metrics.accuracy_score(y_test, y_pred_reg_test)\n",
    "    return svm_train_error, svm_test_error\n",
    "\n",
    "\n",
    "def poly_svm(X,y, X_test, y_test,gamma, c):\n",
    "    clf = svm.SVC(gamma=gamma, kernel='poly', C=c)\n",
    "    clf.fit(X, y)\n",
    "    pred_train_reg=clf.predict(X)\n",
    "    y_pred_reg_test=clf.predict(X_test)\n",
    "    svm_train_error = 1-metrics.accuracy_score(y, pred_train_reg)\n",
    "    svm_test_error = 1-metrics.accuracy_score(y_test, y_pred_reg_test)\n",
    "    return svm_train_error, svm_test_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_gamma_plot(X,y, X_test, y_test, C):\n",
    "    error_lssvm_test = np.zeros(7)\n",
    "    error_lssvm_train= np.zeros(7)\n",
    "    error_linsvm_train = np.zeros(7)\n",
    "    error_linsvm_test = np.zeros(7)\n",
    "    error_rbfsvm_train = np.zeros(7)\n",
    "    error_rbfsvm_test = np.zeros(7)\n",
    "    error_polysvm_train = np.zeros(7)\n",
    "    error_polysvm_test = np.zeros(7)\n",
    "\n",
    "    names = ['LS-SVM-Train', 'LS-SVM-Test', 'Rbf-SVM-Train','Rbf-SVM-Test','Linear-SVM-Train','Linear-SVM-Test','Polynomial-SVM-Train','Polynomial-SVM-Test']\n",
    "\n",
    "\n",
    "    gamma = 10**-4\n",
    "    for i in range(7):\n",
    "        error_lssvm_train[i], error_lssvm_test[i] = LSSVM(X,y, X_test, y_test, gamma, C)\n",
    "        error_linsvm_train[i], error_linsvm_test[i] = linear_svm(X,y, X_test, y_test, gamma, C)\n",
    "        error_polysvm_train[i], error_polysvm_test[i] = poly_svm(X,y, X_test, y_test, gamma, C)\n",
    "        error_rbfsvm_train[i], error_rbfsvm_test[i] = rbf_svm(X,y, X_test, y_test, gamma, C)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        gamma = gamma * 10\n",
    "    print(error_lssvm_train)\n",
    "    df = pd.DataFrame(\n",
    "    {'LS-SVM-Train': error_lssvm_train,\n",
    "     'LS-SVM-Test': error_lssvm_test,\n",
    "     'Rbf-SVM-Train': error_rbfsvm_train,\n",
    "     'Rbf-SVM-Test': error_rbfsvm_test,\n",
    "     'Linear-SVM-Train': error_linsvm_train,\n",
    "     'Linear-SVM-Test': error_linsvm_test,\n",
    "     'Polynomial-SVM-Train': error_polysvm_train,\n",
    "     'Polynomial-SVM-Test': error_polysvm_test\n",
    "        \n",
    "    })\n",
    "    df.boxplot()\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    print(df)\n",
    "    percentile_list = pd.DataFrame(\n",
    "    {'lst1Title': error_lssvm_train,\n",
    "     'lst2Title': error_linsvm_train,\n",
    "     'lst3Title': error_polysvm_train\n",
    "    })\n",
    "\n",
    "    \n",
    "def make_c_plot(X,y, X_test, y_test, gamma):\n",
    "    error_lssvm_test = np.zeros(7)\n",
    "    error_lssvm_train= np.zeros(7)\n",
    "    error_linsvm_train = np.zeros(7)\n",
    "    error_linsvm_test = np.zeros(7)\n",
    "    error_rbfsvm_train = np.zeros(7)\n",
    "    error_rbfsvm_test = np.zeros(7)\n",
    "    error_polysvm_train = np.zeros(7)\n",
    "    error_polysvm_test = np.zeros(7)\n",
    "\n",
    "    names = ['LS-SVM-Train', 'LS-SVM-Test', 'Rbf-SVM-Train','Rbf-SVM-Test','Linear-SVM-Train','Linear-SVM-Test','Polynomial-SVM-Train','Polynomial-SVM-Test']\n",
    "\n",
    "\n",
    "    C = 10**-4\n",
    "    for i in range(7):\n",
    "        error_lssvm_train[i], error_lssvm_test[i] = LSSVM(X,y, X_test, y_test, gamma, C)\n",
    "        error_linsvm_train[i], error_linsvm_test[i] = linear_svm(X,y, X_test, y_test, gamma, C)\n",
    "        error_polysvm_train[i], error_polysvm_test[i] = poly_svm(X,y, X_test, y_test, gamma, C)\n",
    "        error_rbfsvm_train[i], error_rbfsvm_test[i] = rbf_svm(X,y, X_test, y_test, gamma, C)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        C = C * 10\n",
    "    print(error_lssvm_train)\n",
    "    df = pd.DataFrame(\n",
    "    {'LS-SVM-Train': error_lssvm_train,\n",
    "     'LS-SVM-Test': error_lssvm_test,\n",
    "     'Rbf-SVM-Train': error_rbfsvm_train,\n",
    "     'Rbf-SVM-Test': error_rbfsvm_test,\n",
    "     'Linear-SVM-Train': error_linsvm_train,\n",
    "     'Linear-SVM-Test': error_linsvm_test,\n",
    "     'Polynomial-SVM-Train': error_polysvm_train,\n",
    "     'Polynomial-SVM-Test': error_polysvm_test\n",
    "        \n",
    "    })\n",
    "    df.boxplot()\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "make_gamma_plot(X,y, X_test,y_test,1)\n",
    "make_c_plot(X,y, X_test,y_test,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import copy\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Download data\n",
    "\n",
    "tmp = sklearn.datasets.fetch_california_housing()\n",
    "\n",
    "num_samples   = tmp['data'].shape[0]\n",
    "feature_names = tmp['feature_names']\n",
    "y = tmp['target']\n",
    "X = tmp['data']\n",
    "\n",
    "data = {}\n",
    "for n, feature in enumerate(feature_names):\n",
    "    data[feature] = tmp['data'][:,n]\n",
    "    \n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# Create stumps\n",
    "\n",
    "# bin the data by proportion, 10% in each bin\n",
    "bins = {}\n",
    "bin_idx = (np.arange(0,1.1,0.1)*num_samples).astype(np.int16)\n",
    "bin_idx[-1] = bin_idx[-1]-1\n",
    "for feature in (feature_names):\n",
    "    bins[feature] = np.sort(data[feature])[bin_idx]\n",
    "\n",
    "# decision stumps as weak classifiers\n",
    "# 0 if not in bin, 1 if in bin\n",
    "stumps = {}\n",
    "for feature in feature_names:\n",
    "    stumps[feature] = np.zeros([num_samples,len(bins[feature])-1])\n",
    "    for n in range(len(bins[feature])-1):\n",
    "        stumps[feature][:,n] = data[feature]>bins[feature][n]\n",
    "\n",
    "# stack the weak classifiers into a matrix\n",
    "H = np.hstack([stumps[feature] for feature in feature_names])\n",
    "H = np.hstack([np.ones([num_samples,1]),H])\n",
    "# prepare the vector for storing weights\n",
    "alphas = np.zeros(H.shape[1])\n",
    "bins[feature_names[1]]\n",
    "\n",
    "\n",
    "# ### AdaBoost\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "num_iterations = 30\n",
    "MSE = np.zeros(num_iterations) # track mean square error\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    f = np.dot(H,alphas)  # the current f(x)\n",
    "    r = y-f; MSE[iteration] = np.mean(r**2) # r = residual\n",
    "    s = 2/20640 * np.absolute(np.dot(r,H))          \n",
    "    idx = np.argmax(s)# optimal direction to move in\n",
    "    alphas[idx] = alphas[idx] + (np.dot(H[:,idx],r) / np.dot(H[:,idx].T, H[:,idx])) # amount to move in optimal direction\n",
    "\n",
    "\n",
    "# ### Plot Results\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "alphasf = {}\n",
    "start = 1\n",
    "for feature in feature_names:\n",
    "    alphasf[feature] = alphas[start:(start+stumps[feature].shape[1])]\n",
    "    start = start + stumps[feature].shape[1]\n",
    "alphasf['mean'] = alphas[0]\n",
    "\n",
    "alphasf[feature_names[0]]\n",
    "stumps[feature_names[0]].shape\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "for feature in feature_names:\n",
    "    plt.close(\"all\")\n",
    "    plt.plot(data[feature],y-np.mean(y),'.',alpha=0.5,color=[0.9,0.9,0.9])\n",
    "   \n",
    "    plt.title(feature)\n",
    "    plt.xlim([bins[feature][0],bins[feature][-2]])\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('contribution to house price')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "for feature in feature_names:\n",
    "    plt.close(\"all\")\n",
    "    plt.plot(data[feature],y-np.mean(y),'.',alpha=0.5,color=[0.9,0.9,0.9])\n",
    "    plt.plot(data[feature],np.dot(stumps[feature], alphasf[feature]) -np.mean(np.dot(stumps[feature], alphasf[feature])), \".\", alpha = .5)\n",
    "    #plt.step(bins[feature][:10], alphasf[feature])\n",
    "    \n",
    "\n",
    "    plt.title(feature)\n",
    "    plt.xlim([bins[feature][0],bins[feature][-2]])\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('contribution to house price')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ### Variable Importance\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "def create_bins(X, y, data):\n",
    "    # bin the data by proportion, 10% in each bin\n",
    "    bins = {}\n",
    "    bin_idx = (np.arange(0,1.1,0.1)*num_samples).astype(np.int16)\n",
    "    bin_idx[-1] = bin_idx[-1]-1\n",
    "    for feature in (feature_names):\n",
    "        bins[feature] = np.sort(data[feature])[bin_idx]\n",
    "\n",
    "    # decision stumps as weak classifiers\n",
    "    # 0 if not in bin, 1 if in bin\n",
    "    stumps = {}\n",
    "    for feature in feature_names:\n",
    "        stumps[feature] = np.zeros([num_samples,len(bins[feature])-1])\n",
    "        for n in range(len(bins[feature])-1):\n",
    "            stumps[feature][:,n] = data[feature]>bins[feature][n]\n",
    "\n",
    "    # stack the weak classifiers into a matrix\n",
    "    H = np.hstack([stumps[feature] for feature in feature_names])\n",
    "    H = np.hstack([np.ones([num_samples,1]),H])\n",
    "    # prepare the vector for storing weights\n",
    "    alphas = np.zeros(H.shape[1])\n",
    "    bins[feature_names[1]]\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    num_iterations = 30\n",
    "    MSE = np.zeros(num_iterations)\n",
    "    for iteration in range(num_iterations):\n",
    "        f = np.dot(H,alphas)  # the current f(x)\n",
    "        r = y-f; MSE[iteration] = np.mean(r**2) # r = residual\n",
    "        s = 2/20640 * np.absolute(np.dot(r,H))          \n",
    "        idx = np.argmax(s)# optimal direction to move in\n",
    "        alphas[idx] = alphas[idx] + (np.dot(H[:,idx],r) / np.dot(H[:,idx].T, H[:,idx])) # amount to move in optimal direction\n",
    "    print(MSE[29])\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "for feature in feature_names:\n",
    "    new_data = copy.deepcopy(data)\n",
    "    new_data[feature] = np.random.permutation(new_data[feature])\n",
    "    create_bins(X,y, new_data)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ### Boosted Decision Trees\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "clf = GradientBoostingRegressor(loss=\"ls\")\n",
    "clf.fit(X,y)\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "plt.close(\"all\")\n",
    "plt.figure(figsize=[10,10])\n",
    "ax = plt.gca()\n",
    "plot_partial_dependence(clf, X, feature_names, feature_names, n_cols=3, ax=ax) \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### Linear Regression\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "clf2 = LinearRegression()\n",
    "clf2.fit(X,y)\n",
    "\n",
    "\n",
    "# #### Comparison in MSE\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "np.mean((y-clf2.predict(X))**2)\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "np.mean((y-clf.predict(X))**2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
